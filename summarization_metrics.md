---
layout: default
title: Summarization metrics
---

Recently, I started working on summarization metrics, or the metrics for text generation (natural language generation, NLG), including summaarization, simplification, and translation, at large. 

[An introductory video](https://www.youtube.com/watch?v=8ZOLdySNuMQ)

# Background, terminology, and references 
* Summarization is a task under the banner of Natural Language Generation (NLG). The input is called a __document__ and the output is called a __summary__. The input is usually much longer than the output. 
* To judge the quality of a summary generated by a summarizer (such a summary is called a __system summary__), one approach is to compare the system summary against a __reference summary__, which is usually written by human. This approach is known as the __reference-based approach__ (and thus __reference-based metrics__) including some prominent ones, such as [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), [BLEURT](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html), [MoverScore](https://github.com/AIPHES/emnlp19-moverscore), [BERTScore](https://github.com/Tiiiger/bert_score). In machne translation, such method includes BLEU.     
* But reference-based approaches are greatly limited by the cost of obtaining human-written reference summaries. (See Section I of [SueNes paper](https://openreview.net/pdf?id=rfGxaxhWr-5)). Hence, in recent years, research is into the __reference-free__ metrics, which directly compare a system summary with the input document, including [BLANC](https://github.com/PrimerAI/blanc), [SummaQA](https://github.com/ThomasScialom/summa-qa), [SUPERT](https://github.com/yg211/acl20-ref-free-eval), [LS-Score](https://github.com/whl97/LS-Score). 
* Mathematically, the two kinds of metrics are functions of two pieces of text: 
  * reference-based: $f(\text{reference summary}, \text{system/generated summary})$, short as $f(\text{ref}, \text{sys})$. 
  * reference-free: $f(\text{document}, \text{system/generated summary})$, short as $f(\text{doc}, \text{sys})$. 
* We have two papers so far on reference-free approaches: 
  * Ge Luo, Hebi Li, Youbiao He and Forrest Sheng Bao, [PrefScore: Pairwise Preference Learning for Reference-free Summarization Quality Assessment](https://openreview.net/pdf?id=BAuigajYY57), COLING 2022
  * Forrest Sheng Bao, Ge Luo, Hebi Li, Cen Chen, Yinfei Yang, Youbiao He, Minghui Qiu, [SueNes: A Weakly Supervised Approach to Evaluating Single-Document Summarization via Negative Sampling](https://aclanthology.org/2022.naacl-main.175/), NAACL 2022

# Datasets
There are two kinds of datasets associated with summarization research, 
1. summarization datasets (pairs of documents and summaries, i.e., `(doc1, sum1), (doc2, sum2), ...`), such as 
   * [CNN/Dailymail](https://www.tensorflow.org/datasets/catalog/cnn_dailymail) (CNNDM)
   * [BigPatent](https://www.tensorflow.org/datasets/catalog/big_patent)
   * [Billsum](https://www.tensorflow.org/datasets/catalog/billsum)
   * [Newsroom](https://www.tensorflow.org/datasets/catalog/newsroom) (which is a not a good one as a summary is just one sentence)
   * [ScientificPapers](https://www.tensorflow.org/datasets/catalog/scientific_papers) (it has two subsets, arXiv and PubMed)
   
   and 
2. summarization evaluation datasets (tuples of one document and multiple summaries generated by different summarizers, and human ratings for each summary, i.e., `(doc1, sum1A, sum1B, ..., rate1A, rate1B, ...), (doc2, sum2A, sum2B, ..., rate2A, rate2B, ...), ...`), such as 
   * [TAC2010](https://tac.nist.gov//2010/) (I cannot give Non-ISU personnel access to. This is US gov data.)
   * [RealSumm](https://github.com/neulab/REALSumm)
   * [Newsroom](https://github.com/lil-lab/newsroom/) 

In summarization evaluation/quality studies, the second type of datasets always serve as **test sets** because human evaluation is the groundtruth on summary qualities. 

# Supervised approach is hard 
Because a summarization evaluation dataset (TAC2010, RealSumm, Newsroom) is usually very small, say 100 samples, it is prone to overfitting to train a model using human ratings as targets/labels on such a dataset. Instead, an unsupervised approach, like ROUGE, BLEU or BERTScore, or a weak/self/semi-surpervised approach, like SueNes or BLUERT, is preferred. 

# Test area
$$ a \times b $$
