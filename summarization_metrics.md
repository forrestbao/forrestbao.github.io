---
layout: default
title: Summarization metrics
---

Recently, I started working on summarization metrics, or the metrics for text generation (natural language generation, NLG), including summaarization, simplification, and translation, at large. 

## [View this page with proper math and diagrams in Github](https://github.com/forrestbao/forrestbao.github.io/blob/main/summarization_metrics.md)



### Table of contents 
* [Summarization vs. Summarization evaluation/metrics](#summarization-vs-summarization-evaluationmetrics)
* [Reference-based vs. reference-free summary evaluation/metrics](#reference-based-vs-reference-free-summary-evaluationmetrics)
* [Datasets](#datasets)
* [Why not supervised approaches](#supervised-approach-is-hard)

# Summarization vs. Summarization evaluation/metrics

They sounds similar but they differ hugely. 

* Summarization: 
  - By: a **summarizer**, also called a **system**
  - Input: a **document**
  - Output: a **summary**, which is usually much shorter than the document. Also called a **system summary**. 

  ```mermaid
  graph LR;
      A(Document) --> B((Summarizer)) --> C(Summary) ;
  ```
* Summary evaluation/metric:
  - Input: 
    1. a system summary to be judged
    2. the corresponding document, OR a **reference summary**
  - Output: a score. 
  ```mermaid
    graph LR;
      A(System Summary) --> B((metric)) --> C(score) ;
      D(Document <br> OR <br> Reference Summary) --> B ;
  ```

# Reference-based vs. reference-free summary evaluation/metrics

### [An introductory video](https://www.youtube.com/watch?v=8ZOLdySNuMQ)

Depending on the 2nd input of summary evaluation, there are two branches:
* **Reference-based**: Compares the system summary against a reference summary written by a human. 
  - Pros: Accurate
  - Cons: Laborious to obtain reference summaries. 
  - Examples: [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)), [BLEURT](https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html), [MoverScore](https://github.com/AIPHES/emnlp19-moverscore), [BERTScore](https://github.com/Tiiiger/bert_score)
  ```mermaid
    graph LR;
      A(System Summary) --> B((metric)) --> C(score) ;
      D(Document) --> E{human} --> F(Reference Summary) --> B;
      D --> G((summarizer)) -->A;
  ```
  - Mathematically, $f(\text{reference summary}, \text{system/generated summary})$, short as $f(\text{ref}, \text{sys})$. 
* **Reference-free**: Compares the system summary directly against the document
  - Pros: Not relying on reference summaries, which are costly to obtain
  - Cons: Less accurate 
  - Examples: [BLANC](https://github.com/PrimerAI/blanc), [SummaQA](https://github.com/ThomasScialom/summa-qa), [SUPERT](https://github.com/yg211/acl20-ref-free-eval), [LS-Score](https://github.com/whl97/LS-Score), [SueNes](https://aclanthology.org/2022.naacl-main.175/), [PrefScore](https://openreview.net/pdf?id=BAuigajYY57)
  ```mermaid
    graph LR;
      A(System Summary) --> B((metric)) --> C(score) ;
      D(Document) --> B;
      D --> G((summarizer)) -->A ;
  ```
  - Mathematically, $f(\text{document}, \text{system/generated summary})$, short as $f(\text{doc}, \text{sys})$. 

# System-level vs. Summary-level evaluation 

# Datasets
There are two kinds of datasets associated with summarization research, 
1. summarization datasets (pairs of documents and summaries, i.e., `(doc1, sum1), (doc2, sum2), ...`), such as 
   * [CNN/Dailymail](https://www.tensorflow.org/datasets/catalog/cnn_dailymail) (CNNDM)
   * [BigPatent](https://www.tensorflow.org/datasets/catalog/big_patent)
   * [Billsum](https://www.tensorflow.org/datasets/catalog/billsum)
   * [Newsroom](https://www.tensorflow.org/datasets/catalog/newsroom) (which is a not a good one as a summary is just one sentence)
   * [ScientificPapers](https://www.tensorflow.org/datasets/catalog/scientific_papers) (it has two subsets, arXiv and PubMed)
   
   and 
2. summarization evaluation datasets (tuples of one document and multiple summaries generated by different summarizers, and human ratings for each summary, i.e., `(doc1, sum1A, sum1B, ..., rate1A, rate1B, ...), (doc2, sum2A, sum2B, ..., rate2A, rate2B, ...), ...`), such as 
   * [TAC2010](https://tac.nist.gov//2010/) (I cannot give Non-ISU personnel access to. This is US gov data.)
   * [RealSumm](https://github.com/neulab/REALSumm)
   * [Newsroom](https://github.com/lil-lab/newsroom/) 
   * [SummEval](https://github.com/Yale-LILY/SummEval)

In summarization evaluation/quality studies, the second type of datasets always serve as **test sets** because human evaluation is the groundtruth on summary qualities. 

# Supervised approach is hard 
Because a summarization evaluation dataset (TAC2010, RealSumm, Newsroom) is usually very small, say 100 samples, it is prone to overfitting to train a model using human ratings as targets/labels on such a dataset. Instead, an unsupervised approach, like ROUGE, BLEU or BERTScore, or a weak/self/semi-surpervised approach, like SueNes or BLUERT, is preferred. 

# Test area
$$ a \times b $$
