---
layout: post
title: "Resources on Parameter-efficient Fine-Tuning (PEFT) of Language Models "
date: 2023-07-06  
author: Forrest Sheng Bao/鮑盛
---

Blogs & review papers
* https://lightning.ai/pages/community/article/understanding-llama-adapters/  (I highly recommend this blog post which has good graphics and code to explain the PEFT methods)
* [Scaling Down to Scale Up: A Guide to Parameter-Efficient Fine-Tuning](https://arxiv.org/pdf/2303.15647.pdf) 

Papers of methods:
* Soft prompting -- Add dimensions at certain layers: 
  * [Prompt Tuning, Lester et al., Google Research](https://arxiv.org/pdf/2104.08691.pdf)
  * [Prefix-tuning, Li and Liang, Stanford](https://arxiv.org/pdf/2101.00190.pdf)  Prompt Tuning and Prefix Tuning are highly similar. 
  * [P-tuning](https://arxiv.org/pdf/2103.10385.pdf)  More complex than Prefix Tuning or Prompt Tuning. Soft prompting on every part in the prompt template. 
  * [P-tuning v2](https://arxiv.org/pdf/2110.07602.pdf) Adding soft prompts to each layer, instead of only the input one as in the above approaches
* Reparametrization -- kinda like adding dimensions at certain layers but much fewer parameters. 
  * [LoRA](https://arxiv.org/pdf/2106.09685.pdf)  
* Adding layers
  * [Adapter: Parameter-Efficient Transfer Learning for NLP](https://arxiv.org/pdf/1902.00751.pdf)  Use more fully-connected layers in a transformer unit.
* Mixed: 
  * [LLAMA-Adapter](https://arxiv.org/pdf/2303.16199.pdf) Both prefix tuning and adding layers (attention). They call the idea zero-init attention: Self-attention on the soft prompt + the attention weighted soft prompt gated + concatenated with the raw input.
  * [LLAMA-Adapter v2](https://arxiv.org/pdf/2304.15010.pdf)  Also finetunes the biases. 

Library tutorials:
* [HuggingFace's `peft`](https://huggingface.co/blog/peft)
* [LightingAI's `lit-gpt`](https://lightning.ai/pages/community/finetuning-falcon-efficiently/)  However, I would not recommend `lit-gpt`. `peft` seems more mature and modularized. 
